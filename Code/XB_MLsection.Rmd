---
title: "BST260 Final Project XB Machine Learning Section"
author: "Xiang Bai"
date: "12/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Libraries

```{r, echo = FALSE}

#libraries
library(dplyr)
library(stringr)
library(tidyverse)
library(RCurl)
library(httr)
library(caret)
library(ggplot2)
library(mlbench)
library(Boruta)
library("DataExplorer")
library(ggstatsplot)
library(ggcorrplot)
library(lares)
library(randomForest)
library("faux")
library(skimr)
library(nnet)
library(e1071)
set.seed(260)
```

## Load Preprocess data

You can also embed plots, for example:

```{r}
df_complete = read.csv("../Data/aggregate_pm_census_cdc.csv")
dim(df_complete) # shape of data
head(df_complete) # brief look at data
names(df_complete) # all included variables
```

There are a total of 47 variables in this data that was combined from [multiple sources](https://github.com/steventang26/BST260-Final-Project/blob/master/README.md).
Of this data there are over 3000 counties and resulting dependent variables--which
we are looking at either COVID cases or COVID deaths.

```{r}

# drop all repeated variables and keep wanted 
# keep Population as the correct variable
todrop = c("fips","Admin2","Active", "q_popdensity","Last_Update", "year", "Country_Region", "NAME", "population", "poverty_mcare", "Recovered")


df_prelim <- df_complete %>% 
  select(-todrop) %>% 
  subset(Province_State != "Utah") %>% 
  rename_all(recode, Combined_Key = "County") %>% 
  remove_rownames %>% 
  column_to_rownames(var = "County") %>% 
  select(-Province_State) %>% # Utah reported no cases or deaths info
  mutate(rate = (Deaths / (Population / 100000)))
  
names(df_prelim)
glimpse(df_prelim)
```


### Exploratory Data Analysis

```{r}
#states = rownames(df[which(df["Deaths"] > 10000),]) #points to label
#rownames(df[states,])

plot_intro(df_prelim) # shows data.nas
#ggstatsplot::ggcorrmat(
#  data = df,
#  type = "parametric", 
#  colors = c("darkred", "white", "steelblue")) # change default colors

## too many variables in ugly correlation plot

  #geom_text(label = rownames(df[states,]))

```

```{r}

# dropping these variabls as redundant with age_pct variables avoid collinearity
todropmore = c("mid_pecent", "prime_pecent", "older_pecent" ,"young_pecent", "Deaths") 

desvars <- c("rate", "mean_pm25", "smoke", "obese", "poverty", "no_grad", "owner_occupied", "hispanic_pct", "blk_pct", "age_pct_65_plus", "age_pct_45_65", "age_pct_15_44", "population_density", "median_household_income", "median_house_value", "mean_summer_temp", "mean_winter_temp", "mean_summer_rm", "mean_winter_rm")


df <- df_prelim %>% 
  select(-todropmore) %>% 
  select(desvars)


plot_correlation(df)

# Picture of Variables
skimmed <- skim_to_wide(df)
skimmed[, c(1,2,4,5,8:12)]


#ggplot(df, aes(x = Confirmed, y = Deaths)) +
#  geom_point(size = 1) +
#  scale_x_log10() +
#  scale_y_log10() ## not a meaningful plot

corr_cross(df, 
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 15)

# highest correlations with COVID death rates
corr_var(df, # name of dataset
  rate, # name of variable to focus on
  top = 20, # display top 5 correlations
  max_pvalue = 0.05)

#find cases with NAs
df_missing = df[!complete.cases(df), ]
df_missing #showing rows

# removed 1 county with no smoke data
df <- df[!(row.names(df) %in% "Oglala Lakota, South Dakota, US"), ]

plot_intro(df)

```


Preliminary look shows that interestingly PM2.5 values, age above 65yo, and smoking have low correlations with COVID death numbers.

### Feature Selection Explore

#```{r eval= FALSE}

# Define the control using a random forest selection function
control <- rfeControl(functions = rfFuncs, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 2, # number of repeats
                      number = 5) # number of folds

# Independent Variables
x <- df %>%
  select(-rate) %>%
  as.data.frame()

# Target variable
y <- df$rate

# Training: 80%; Test: 20%
set.seed(260)
inTrain <- createDataPartition(y, p = .80, list = FALSE)[,1]
x_train <- x[ inTrain, ]
x_test  <- x[-inTrain, ]
y_train <- y[ inTrain]
y_test  <- y[-inTrain]

dim(x_train)
length(y_train)

# Run RFE
result_rfe1 <- rfe(x = x_train, 
                   y = y_train, 
                   sizes = 10,
                   rfeControl = control)
# ran combination of 10 best predictors


# Print the results
result_rfe1

# Print the selected features
predictors(result_rfe1)




### Machine Learning Analysis `Support Vector Regression`

```{r cache = TRUE}

#split again with deaths variable
index_train<- createDataPartition(y = df$rate, times =1, p=0.8, list = FALSE)
train_set <- slice(df, index_train)
test_set <- slice(df, -index_train)

dim(train_set)
dim(test_set)

## Trying SVR Model
#Regression with SVM
#with top 10 correlated
modelsvm = svm(rate ~ mean_summer_temp + no_grad + median_house_value + median_household_income+ mean_winter_temp+ poverty+ smoke+ blk_pct+ obese+ age_pct_45_65+ mean_pm25, train_set)

#Predict using SVM regression
predYsvm = predict(modelsvm, test_set)

#just to see
test_set$rate[10:20]
predYsvm[10:20]

summary(modelsvm)

W = t(modelsvm$coefs) %*% modelsvm$SV

#Find value of b
b = modelsvm$rho

## RMSE for SVR Model

#Calculate RMSE 
RMSEsvm=rmse(predYsvm,test_set$rate)
RMSEsvm
R2(predYsvm,test_set$rate)
## Tuning SVR model by varying values of maximum allowable error and cost parameter

#Tune the SVM model
OptModelsvm = tune(svm, rate ~ mean_summer_temp + no_grad + median_house_value + median_household_income+ mean_winter_temp+ poverty+ smoke+ blk_pct+ obese+ age_pct_45_65+ mean_pm25, data = train_set, ranges=list(elsilon=seq(0,1,0.5), cost = 1:100))

#Print optimum value of parameters
print(OptModelsvm)

#Plot the performance of SVM Regression model
plot(OptModelsvm)

#Find out the best model
#BstModel=OptModelsvm$best.model

#Predict Y using best model
#PredYBst=predict(BstModel,test_set)

#Calculate RMSE of the best model 
#RMSEBst=rmse(PredYBst,test_set$rate)
#RMSEBst #better RMSE value than untuned model

##Calculate parameters of the Best SVR model

#Find value of W
#Wbest = t(BstModel$coefs) %*% BstModel$SV

#Find value of b
#b_best = BstModel$rho

#RMSE(PredYBst, test_set$rate)
#modelsvm = svm(rate ~ ., train_set)

# not very good performance on the models thru RMSE and Squared values
# simpler approach is to 
#mean(df$rate)

```



```{r}
# determine the variables to run on
# should i run a lasso?






```{r}

```



```{r}

```


```{r}

```


https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7
https://topepo.github.io/caret/train-models-by-tag.html#neural-network
