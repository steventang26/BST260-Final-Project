---
title: "Yu Fu Random Forest"
author: "Yu Fu"
date: "12/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries

```{r, warning = F, include = F}
#libraries
library(dplyr)
library(stringr)
library(tidyverse)
library(splitstackshape)
library(httr)
library(caret)
library(ggplot2)
library(mlbench)
library(Boruta)
library(DataExplorer)
library(ggstatsplot)
library(ggcorrplot)
library(lares)
library(randomForest)
library(MASS)
library("faux")
library(skimr)
library(nnet)
library(e1071)
```

## Load data and data wrangling

We are using two COVID-19 dataset, the first one contains data up until 04/15/2021 and the second one contains data through the end our of study period 10/31/21.

Since Utal reported incomplete information on COVID-19 confirmed cases and deaths, we will exclude Utah from our analysis. We further excluded county with missing value.

```{r, warning = FALSE, results='hide'}
## load the data up to 10/31 and 04/15, and create a variable for county-level COVID-19 death rate
aggregate_pm_census_cdc_210415 <- read_csv("Data/aggregate_pm_census_cdc_210415.csv")
aggregate_pm_census_cdc_211031 <- read_csv("Data/aggregate_pm_census_cdc_211031.csv")

df_complete_1031 <- aggregate_pm_census_cdc_211031 %>% mutate(rate = (Deaths / (Population / 100000)))
names(df_complete_1031) 

df_complete_0415 <- aggregate_pm_census_cdc_210415 %>% mutate(rate = (Deaths / (Population / 100000)))
names(df_complete_0415)
```

## Load data
```{r, warning = FALSE, results='hide'}
## load preprocessed data by XB, create a variable for county-level COVID-19 death rate
df_complete <- aggregate_pm_census_cdc_211031 %>% mutate(rate = (Deaths / (Population / 100000)))
names(df_complete)

## drop redundant variables
todrop <- c("fips","Admin2","Recovered","Active", "q_popdensity","Last_Update", "year", "Country_Region", "NAME", "population", "mid_pecent", "prime_pecent", "older_pecent" ,"young_pecent")

df_complete_1031 <- df_complete_1031[ , !(names(df_complete_1031) %in% todrop)]
df_complete_0415 <- df_complete_0415[ , !(names(df_complete_0415) %in% todrop)]

df_1031 <- df_complete_1031 %>% 
  subset(Province_State != "Utah") %>% 
  rename_all(recode, Combined_Key = "County") %>% 
  remove_rownames %>% 
  column_to_rownames(var = "County")

df_0415 <- df_complete_0415 %>% 
  subset(Province_State != "Utah") %>% 
  rename_all(recode, Combined_Key = "County") %>% 
  remove_rownames %>% 
  column_to_rownames(var = "County")

##view data
head(df_1031)
names(df_1031)
glimpse(df_1031)

head(df_0415)
names(df_0415)
glimpse(df_0415)
```

```{r, warning = F, results='hide'}
## Remove NAs
which(is.na(df_0415$rate))
which(is.na(df_1031$rate))
## In both dataset, row 2379 has missing value for opulation and rate.
df_0415<- df_0415[-2379,]
df_1031<- df_1031[-2379,]
```

## Random Forest Model with data before vaccination

Since Random Forests can calculate feature importance (which predictors are the most predictive) and outperform linear models when relationship between outcome and predictors is complex and non-linear, we used Random Forest models to predict COVID-19 mortality rate and explore the importance of predictors in our model.

### Step 1: Split data

We used a 80% training, 20% test set split. We have 2464 observations in the training dataset and 614 observations in the test dataset.

```{r}
set.seed(1)

index_train_0415<- createDataPartition(y = df_0415$rate, times =1, p=0.8, list = FALSE)
train_set_0415 <- slice(df_0415, index_train_0415)
test_set_0415 <- slice(df_0415, -index_train_0415)
dim(train_set_0415)
dim(test_set_0415)
```

### Step 2: Machine Learning Analysis

We fitted a Random Forest by using the `randomForest()` function to the training set that predicts `rate` based on all of the other variables. `mtry = 18` indicates all 18 predictors should be considered for each split of the tree - in other words, bagging should be done.

```{r}
set.seed(1)

fit_rf_1 <- randomForest(rate ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set_0415, mtry = 18, importance = TRUE)
fit_rf_1
```

How well does this model perform?

```{r}
preds_rf_1 = predict(fit_rf_1, newdata = test_set_0415)
plot(preds_rf_1, test_set_0415$rate, xlab = "predicted", ylab = "test")
abline(0,1)
sqrt(mean((preds_rf_1 - test_set_0415$rate)^2))
```

With Random Forest (`mtry = 18`), we got a RMSE of 4.66. To imporve prediction performance, we fitted another model with a smaller value for `mtry`. The default for regression trees is p/3, so we used `mtry = 6`.

```{r}
fit_rf_2 <- randomForest(rate ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set_0415, mtry = 6, importance = TRUE)
fit_rf_2
```

How well does model 2 perform?

```{r}
preds_rf_2 = predict(fit_rf_2, newdata = test_set_0415)
plot(preds_rf_2, test_set_0415$rate, xlab = "predicted", ylab = "test")
abline(0,1)
sqrt(mean((preds_rf_2 - test_set_0415$rate)^2))
```

With Random Forest (`mtry = 6`), we got a RMSE of 4.62, which is slightly better than model 1 with `mtry = 18`. We would like to examine variable importance from the random forest model.

```{r}
library(knitr)
variable_importance_0415 <- importance(fit_rf_2) 
tmp_0415 <- tibble(feature = rownames(variable_importance_0415),
                  Gini = variable_importance_0415[,1]) %>%
                  arrange(desc(Gini))
kable(tmp_0415[1:10,])

tmp_0415 %>% filter(Gini > 20) %>%
        ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
        geom_bar(stat='identity') +
        coord_flip() + xlab("Feature") +
        theme(axis.text=element_text(size=8))
```
      
The top 10 important predictors are: median house value, average summer temperature, % 45-65 years of age, average pm 2.5, average winter temperature, % >= 65 years of age, % smokers, average summer relative humidity, median household income, and % 15-44 years of age.

Based on predictor importance, we fitted a Random Forest model to the training set that predicts `rate` based `median_house_value`, `mean_summer_temp`, `age_pct_45_65`, `mean_pm25`, `mean_winter_temp`, `age_pct_65_plus`, and `smoke`. There are 7 predictors, so we tried `mtry = 2`

```{r}
set.seed(1)
fit_rf_3 <- randomForest(rate ~ mean_summer_temp + median_house_value + mean_pm25 + age_pct_65_plus + age_pct_45_65 + mean_winter_temp + smoke, data = train_set_0415, mtry = 2, importance = TRUE)
fit_rf_3
```

How well does model 3 perform?

```{r}
preds_rf_3 = predict(fit_rf_3, newdata = test_set_0415)
plot(preds_rf_3, test_set_0415$rate, xlab = "predicted", ylab = "test")
abline(0,1)
sqrt(mean((preds_rf_3 - test_set_0415$rate)^2))
```

For model 3 (`mtry = 2`) with only the top 7 most important predictors, we got a RMSE of 4.54, which is  better than the previous models.

### Random Forest Model with complete data up until 10/31/2021

We explored how widespread availability of vaccines potentially changed the importance of predictors in our models. Therefore, we carried out the following analysis using the complete dataset that contains COVID-19 mortality rate after the introduction of vaccines.

### Step 1: Split data

We used a 80% training, 20% test set split. We have 2464 observations in the training dataset and 614 observations in the test dataset.

```{r}
df_1031 <- df_1031 %>%
   mutate(Mortality_bin = as.factor(ifelse(rate >= median(rate), 1, 0)))

set.seed(1)

index_train_1031 <- createDataPartition(y = df_1031$rate, times =1, p=0.8, list = FALSE)
train_set_1031 <- slice(df_1031, index_train_1031)
test_set_1031 <- slice(df_1031, -index_train_1031)
dim(train_set_1031)
dim(test_set_1031)
```

### Step 2: Machine Learning Analysis

We fitted a Random Forest model to the training set that predicts `rate` based on all of the other variables. Still, we used `mtry = 18`, which indicates all 18 predictors should be considered for each split of the tree.

```{r}
set.seed(1)

fit_rf_4 <- randomForest(rate ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set_1031, mtry = 18, importance = TRUE)
fit_rf_4
```

```{r}
preds_rf_4 = predict(fit_rf_4, newdata = test_set_1031)
plot(preds_rf_4, test_set_1031$rate, xlab = "predicted", ylab = "test")
abline(0,1)
sqrt(mean((preds_rf_4 - test_set_1031$rate)^2))
```

When performing the random forest model (`mtry = 18`) fitting the dataset that contains COVID-19 mortality up until 10/31/2021, we got a RMSE of 5.13. So, we fitted another model with a smaller value of `mtry = 6`.

```{r}
set.seed(1)
fit_rf_5 <- randomForest(rate ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set_1031, mtry = 6, importance = TRUE)
fit_rf_5
```

```{r}
preds_rf_5 = predict(fit_rf_5, newdata = test_set_1031)
plot(preds_rf_5, test_set_1031$rate, xlab = "predicted", ylab = "test")
abline(0,1)
sqrt(mean((preds_rf_5 - test_set_1031$rate)^2))
```

With Random Forest (`mtry = 6`), we got a RMSE of 5.09, which performs better than the model with `mtry = 18`. So, we would like to see the predictor importance of this model

```{r}
library(knitr)
variable_importance_1031 <- importance(fit_rf_5) 
tmp_1031 <- tibble(feature = rownames(variable_importance_1031),
                  Gini = variable_importance_1031[,1]) %>%
                  arrange(desc(Gini))
kable(tmp_1031[1:10,])

tmp_1031 %>% filter(Gini > 20) %>%
        ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
        geom_bar(stat='identity') +
        coord_flip() + xlab("Feature") +
        theme(axis.text=element_text(size=8))
```
      
Now, the top 10 important predictors have changed: average summer temperature, median house value, % smokers, % >= 65 years of age, median household income, % less than high school education, average pm 2.5, average winter temperature, % 45-65 years of age, % 15-44 years of age, and average summer relative humidity. 

It is important to note that the rank of `mean_pm25` has dropped when using the dataset that contains information after the introduction of vaccines. While environmental and socioeconomic factors, such as summer temperature and house values, continue to play an important role in predicting COVID-19 mortality rate, the impact of pm 2.5 seems to be alleviated after when taking vaccination into account. Therefore, it is possible that vaccaination has mitigated the negative effect of the long-term pm 2.5 exposure on COVID-19 mortality.

We fitted another moodel with the top 7 important predictors:

```{r}
set.seed(1)
fit_rf_6 <- randomForest(rate ~ mean_summer_temp + median_house_value + smoke + age_pct_65_plus + median_household_income +  no_grad + mean_pm25, data = train_set_1031, mtry = 2, importance = TRUE)
fit_rf_6
```

```{r}
preds_rf_6 = predict(fit_rf_6, newdata = test_set_1031)
plot(preds_rf_6, test_set_1031$rate, xlab = "predicted", ylab = "test")
abline(0,1)
sqrt(mean((preds_rf_6 - test_set_1031$rate)^2))
```

There an increase in RMSE (5.25) when using the top 7 important predictors.

### Random Forest Model with binary outcome (using complete data up until 10/31/2021)

To better understand variable importance and improve model performance, we fitted a Random Forest model to the training set that predicts `Mortality_bin` based on all of the other variables. Then, we used the predict function to get probability estimates for the test set, and convert the probabilities to predicted response labels (use a cutoff of 0.5). Still, we used `mtry = 18`.

```{r}
set.seed(1)

fit_rf_7 <- randomForest(Mortality_bin ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set_1031, mtry = 18, importance = TRUE)
```

```{r}
preds_rf_7 = predict(fit_rf_7, newdata = test_set_1031, type = "prob")[,2]
y_hat_rf_7 = factor(ifelse(preds_rf_7 > 0.5, 1, 0))
confusionMatrix(data = as.factor(y_hat_rf_7), reference = as.factor(test_set_1031$Mortality_bin), positive = "1")
```

Model 7 (`mtry = 18`) had an overall accuracy of 75.7% with a sensitivity of 75.6% and specificity of 75.9%. We tried another model to predict the binary outcome `Mortality_bin` with `mtry = 6`.

```{r}
set.seed(1)
fit_rf_8 <- randomForest(Mortality_bin ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set_1031, mtry = 6, importance = TRUE)
```

```{r}
preds_rf_8 = predict(fit_rf_8, newdata = test_set_1031, type = "prob")[,2]
y_hat_rf_8 = factor(ifelse(preds_rf_8 > 0.5, 1, 0))
confusionMatrix(data = as.factor(y_hat_rf_8), reference = as.factor(test_set_1031$Mortality_bin), positive = "1")
```

Model 8 (`mtry = 6`) had an overall accuracy of 76.9% with a sensitivity of 76.2% and specificity of 77.5%. There's an increase in overall accuracy as well as in sensitivity and specificity. We further explored variable importance of this model.

```{r}
library(knitr)
variable_importance_m8 <- importance(fit_rf_8) 
tmp_m8 <- tibble(feature = rownames(variable_importance_m8),
                  Gini = variable_importance_m8[,1]) %>%
                  arrange(desc(Gini))
kable(tmp_m8[1:10,])

tmp_m8 %>% filter(Gini > 8.7) %>%
        ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
        geom_bar(stat='identity') +
        coord_flip() + xlab("Feature") +
        theme(axis.text=element_text(size=8))
```
      
The top 10 important predictors are: average summer temperature, median house value, % smokers, % less than high school education, % hispanic, median household income, % >= 65 years of age, 
% 45-65 years of age, population density, and average pm 2.5. 

For the model using binary outcome, we observed that the rank of `mean_pm25` has dropped significantly. For classification, the importance of pm 2.5 has been greatly reduced. 

Then, we used variables that are the most predictive of the outcome to fit the following model: 

```{r}
set.seed(1)
fit_rf_9 <- randomForest(Mortality_bin ~ mean_summer_temp + median_house_value + smoke + no_grad + hispanic_pct + median_household_income + age_pct_45_65 + age_pct_65_plus + population_density + mean_pm25, data = train_set_1031, mtry = 3, importance = TRUE)
```

```{r}
preds_rf_9 = predict(fit_rf_9, newdata = test_set_1031, type = "prob")[,2]
y_hat_rf_9 = factor(ifelse(preds_rf_9 > 0.5, 1, 0))
confusionMatrix(data = as.factor(y_hat_rf_9), reference = as.factor(test_set_1031$Mortality_bin), positive = "1")
```

Using important predictors, model 9 had an overall accuracy of 77.2% with a sensitivity of 75.9% and specificity of 78.5%. Overall, this is the best model for classification. 

