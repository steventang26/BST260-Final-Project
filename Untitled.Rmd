---
title: "Machine Learning-Yu Fu"
author: "Yu Fu"
date: "12/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries
```{r}
#libraries
library(dplyr)
library(stringr)
library(tidyverse)
library(splitstackshape)
library(httr)
library(caret)
library(ggplot2)
library(mlbench)
library(Boruta)
library(DataExplorer)
library(ggstatsplot)
library(ggcorrplot)
library(lares)
library(randomForest)
library(MASS)
library("faux")
library(skimr)
library(nnet)
library(e1071)
set.seed(260)
```

## Load data
```{r}
## load preprocessed data by XB, create a variable for county-level COVID-19 death rate
df_complete <- aggregate_pm_census_cdc_211031 %>% mutate(rate = (Deaths / (Population / 100000)))
names(df_complete)

## drop redundant variables
todrop <- c("fips","Admin2","Recovered","Active", "q_popdensity","Last_Update", "year", "Country_Region", "NAME", "population", "mid_pecent", "prime_pecent", "older_pecent" ,"young_pecent")

df_complete <- df_complete[ , !(names(df_complete) %in% todrop)]

df <- df_complete %>% 
  subset(Province_State != "Utah") %>% 
  rename_all(recode, Combined_Key = "County") %>% 
  remove_rownames %>% 
  column_to_rownames(var = "County")
# Utah reported no cases or deaths info

##view data
head(df)
names(df)
glimpse(df)
```
## Remove NAs
```{r}
which(is.na(df$rate))
## Row 2379, Population and rate is missing.
df <- df[-2379,]
```

## Split data
```{r}
set.seed(1)

index_train<- createDataPartition(y = df$rate, times =1, p=0.8, list = FALSE)
train_set <- slice(df, index_train)
test_set <- slice(df, -index_train)
dim(train_set)
dim(test_set)
```

## Machine Learning model

```{r}
## We fit a Random Forest by using the `randomForest()` function. Here, `mtry = 18` indicates all 18 predictors should be considered for each split of the tree - in other words, bagging should be done.
set.seed(1)
fit_rf_1 <- randomForest(rate ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set, mtry = 18, importance = TRUE)
fit_rf_1

##How well does this model perform?
preds_bag = predict(fit_rf_1, newdata = test_set)
plot(preds_bag, test_set$rate)
abline(0,1)
mean((preds_bag - test_set$rate)^2) 
#With Random Forest (mtry = 18), we get an MSE of 26.3.
```

```{r}
## RF model with mtry = p/3 = 6 (the de)
set.seed(1)
fit_rf_2 <- randomForest(rate ~ mean_pm25 + smoke + obese + poverty + no_grad + owner_occupied + hispanic_pct + blk_pct + age_pct_15_44 + age_pct_45_65 + age_pct_65_plus + population_density + median_house_value + median_household_income + mean_winter_temp + mean_summer_temp + mean_winter_rm + mean_summer_rm, data = train_set, mtry = 6, importance = TRUE)
fit_rf_2

##How well does this model perform?
preds_rf_2 = predict(fit_rf_2, newdata = test_set)
plot(preds_rf_2, test_set$rate)
abline(0,1)
mean((preds_rf_2 - test_set$rate)^2)
# With Random Forest (mtry = 6), we get an MSE of 25.9.
```


```{r}
library(knitr)
variable_importance <- importance(fit_rf_2) 
tmp <- tibble(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
                  arrange(desc(Gini))
kable(tmp[1:10,])

## The top 10 important predictors are: average summer temperature, median house value, %smokers, %>= 65 years of age, median household income, no_grad (% less than high school???), average pm2.5, average winter temperature...

tmp %>% filter(Gini > 20) %>%
        ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
        geom_bar(stat='identity') +
        coord_flip() + xlab("Feature") +
        theme(axis.text=element_text(size=8))
```
      






